{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,subprocess\n",
    "workdir='/mnt/Data1/ribli/expression_code/modelling/'\n",
    "subprocess.call(['mkdir',workdir])\n",
    "os.chdir(workdir)\n",
    "\n",
    "#theano gpu\n",
    "os.environ['THEANO_FLAGS']='device=gpu'\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('../my_modules')\n",
    "#from loading_utils import read_my_data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def read_my_data(fname,**kwargs):\n",
    "    \"\"\"Load my data from file into np.arrays.\n",
    "    \n",
    "    I had to use garbage collector, because pandas read_csv leaves garbage around.\n",
    "    \"\"\"\n",
    "    \n",
    "    #load data\n",
    "    print \"Loading data... \"\n",
    "    x=pd.read_csv(fname,sep='\\t',header=None)\n",
    "    \n",
    "    # for some reason not everything is cleaned up\n",
    "    #when using the pandas read_csv\n",
    "    gc.collect()\n",
    "    \n",
    "    #probe_id=x[0]\n",
    "    #y=x.iloc[:,-1].values.astype(np.int8)\n",
    "    #x=x.iloc[:,1:-1].values.astype(np.int8)\n",
    "    #return probe_id,x,y\n",
    "    \n",
    "    return x[0],x.iloc[:,1:-1].values.astype(np.int8),x.iloc[:,-1].values.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "(234044, 600)\n"
     ]
    }
   ],
   "source": [
    "train_id,train_x,train_y = read_my_data(fname='../prepare_data/naive_feat_vect.csv')\n",
    "print train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make it image like\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(np.concatenate([train_x.flatten()]))\n",
    "train_x=lb.transform(train_x.flatten()).reshape((-1,1,600,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y=(0.5*(np.sign(train_y-np.median(train_y))+1)).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D,MaxPooling2D\n",
    "\n",
    "input_dim=train_x.shape[2]\n",
    "activation='relu'\n",
    "loss='binary_crossentropy'\n",
    "optimizer='adadelta'\n",
    "init='uniform'\n",
    "pool_size=(8,1)\n",
    "window_size=5\n",
    "dense_n=64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Convolution layer 1\n",
    "model.add(Convolution2D(20,window_size,5, border_mode='valid',input_shape=(1,input_dim,5)))\n",
    "model.add(Activation(activation))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "#Convolution layer 2\n",
    "model.add(Convolution2D(50,window_size,1, border_mode='valid'))\n",
    "model.add(Activation(activation))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "#Dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dense_n,activation=activation))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#final layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#compile model\n",
    "model.compile(loss=loss,optimizer=optimizer,class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "import time\n",
    "def fit_keras_model(model,train_x,train_y,test_x,test_y,validation_split=0.05):\n",
    "    start=time.time()\n",
    "    \n",
    "    #callbacks\n",
    "    best_model=ModelCheckpoint('best_model',save_best_only=True,verbose=1)\n",
    "    early_stop=EarlyStopping(patience=7,verbose=1)\n",
    "    \n",
    "    #train it\n",
    "    callb_hist=model.fit(train_x,train_y,nb_epoch = 100 ,verbose=1,\n",
    "                        validation_split=validation_split,\n",
    "                         show_accuracy=True,\n",
    "                        callbacks=[best_model,early_stop])\n",
    "    #predict\n",
    "    model.load_weights('best_model')\n",
    "    train_pred=model.predict_classes(train_x,verbose=1).ravel()\n",
    "    test_pred=model.predict_classes(test_x,verbose=1).ravel()\n",
    "\n",
    "    #check errors\n",
    "    print 'train score:',list((train_pred==train_y)).count(True)/float(len(train_y))\n",
    "    print 'test score:',list((test_pred==test_y)).count(True)/float(len(test_y))\n",
    "\n",
    "    print 'It took:',time.time()-start    \n",
    "    return train_pred,test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 152000 samples, validate on 38000 samples\n",
      "Epoch 1/100\n",
      "152000/152000 [==============================] - 48s - loss: 0.6790 - acc: 0.5672 - val_loss: 0.6756 - val_acc: 0.5699\n",
      "Epoch 00000: val_loss improved from inf to 0.67559, saving model to best_model\n",
      "Epoch 2/100\n",
      "152000/152000 [==============================] - 48s - loss: 0.6640 - acc: 0.5963 - val_loss: 0.6491 - val_acc: 0.6206\n",
      "Epoch 00001: val_loss improved from 0.67559 to 0.64909, saving model to best_model\n",
      "Epoch 3/100\n",
      "152000/152000 [==============================] - 48s - loss: 0.6558 - acc: 0.6122 - val_loss: 0.6488 - val_acc: 0.6129\n",
      "Epoch 00002: val_loss improved from 0.64909 to 0.64881, saving model to best_model\n",
      "Epoch 4/100\n",
      "152000/152000 [==============================] - 48s - loss: 0.6473 - acc: 0.6233 - val_loss: 0.6312 - val_acc: 0.6430\n",
      "Epoch 00003: val_loss improved from 0.64881 to 0.63123, saving model to best_model\n",
      "Epoch 5/100\n",
      "152000/152000 [==============================] - 48s - loss: 0.6402 - acc: 0.6311 - val_loss: 0.6334 - val_acc: 0.6423\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 6/100\n",
      "152000/152000 [==============================] - 48s - loss: 0.6355 - acc: 0.6363 - val_loss: 0.6308 - val_acc: 0.6427\n",
      "Epoch 00005: val_loss improved from 0.63123 to 0.63084, saving model to best_model\n",
      "Epoch 7/100\n",
      "152000/152000 [==============================] - 48s - loss: 0.6316 - acc: 0.6404 - val_loss: 0.6253 - val_acc: 0.6468\n",
      "Epoch 00006: val_loss improved from 0.63084 to 0.62533, saving model to best_model\n",
      "Epoch 8/100\n",
      "152000/152000 [==============================] - 48s - loss: 0.6275 - acc: 0.6444 - val_loss: 0.6270 - val_acc: 0.6452\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 9/100\n",
      "152000/152000 [==============================] - 48s - loss: 0.6231 - acc: 0.6482 - val_loss: 0.6277 - val_acc: 0.6478\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 10/100\n",
      "152000/152000 [==============================] - 48s - loss: 0.6184 - acc: 0.6542 - val_loss: 0.6309 - val_acc: 0.6443\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 11/100\n",
      " 71296/152000 [=============>................] - ETA: 22s - loss: 0.6101 - acc: 0.6599"
     ]
    }
   ],
   "source": [
    "N_train=190000\n",
    "N_test=40000\n",
    "\n",
    "train_pred,test_pred=fit_keras_model(\n",
    "    model,train_x[:N_train],train_y[:N_train],\n",
    "    train_x[N_train:N_train+N_test],train_y[N_train:N_train+N_test],\n",
    "    validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
